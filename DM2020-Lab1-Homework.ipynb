{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: NING KUO HSUN\n",
    "\n",
    "Student ID: 107062625\n",
    "\n",
    "GitHub ID: ning7013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2020-Lab1-Master Repo](https://github.com/fhcalderon87/DM2020-Lab1-Master). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2020-Lab1-Master Repo](https://github.com/fhcalderon87/DM2020-Lab1-Master) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n",
    "    - Download the [the new dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). The dataset contains a `sentence` and `score` label. Read the specificiations of the dataset for details. \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Sciki-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be habdled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/fhcalderon87/DM2020-Lab1-Master/blob/master/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (Oct. 22th 11:59 pm, Thursday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 2(Take home)\n",
    "#X.iloc[3]   #third line\n",
    "#X.iloc[:3]  #three line\n",
    "#X.iloc[::3] #every three line\n",
    "#X.iloc[3::] #from thrid line\n",
    "#X.iloc[3::5]#from third line, every five line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 5(Take home)\n",
    "2, 3 and 5 do have value, which are 'NaN', 'None', and '', and are in string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 5(Take home)\n",
    "True words, I didnt see anything changed on X..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 8(Take home)\n",
    "X.category_name.value_counts().plot(kind = 'bar', position = 1,rot = 0, fontsize = 12, figsize = (8,3))\n",
    "X_sample.category_name.value_counts().plot(kind = 'bar', position = 0, color = 'orange' ,rot = 0, fontsize = 12, figsize = (8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 10(Take home)\n",
    "A = X_counts[4].toarray()\n",
    "for i  in range(1,100):\n",
    "    if A[0][i] == 1:\n",
    "        break;\n",
    "count_vect.get_feature_names()[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 11(Take home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 12(Take home)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize = (15,10))\n",
    "\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=60, azim=240)\n",
    "#ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=0, azim=180)\n",
    "#ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=0, azim=0)\n",
    "\n",
    "X_reduced = PCA(n_components = 3).fit_transform(X_counts.toarray())\n",
    "col = ['coral', 'blue', 'black', 'm']\n",
    "\n",
    "for c, category in zip(col, categories):\n",
    "    xs = X_reduced[X['category_name'] == category].T[0]\n",
    "    ys = X_reduced[X['category_name'] == category].T[1]\n",
    "    zs = X_reduced[X['category_name'] == category].T[2]\n",
    "   \n",
    "    ax.scatter(xs, ys, zs, c = c, marker='o')\n",
    "\n",
    "ax.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "ax.set_xlabel('\\nX Label')\n",
    "ax.set_ylabel('\\nY Label')\n",
    "ax.set_zlabel('\\nZ Label')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Most of the plots stay in same space, which may means it is often used words like conjunctions or definite articles.\n",
    "#Each color, or ot say, each catagoriy will has it own often used words, to make the plot stays together, or at least the tendency to stay together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 13(Take home)\n",
    "import plotly.express as px\n",
    "df = pd.DataFrame(dict(frequency=term_frequencies[:], term = count_vect.get_feature_names()[:]))\n",
    "fig = px.bar(df, x = df.term, y = df.frequency)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 14(Take home)\n",
    "df = df.drop(df[df.frequency < 50].index)\n",
    "fig = px.bar(df, x = df.term, y = df.frequency)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 15(Take home)\n",
    "df = pd.DataFrame(dict(frequency=term_frequencies[:], term = count_vect.get_feature_names()[:]))\n",
    "df = df.sort_values(by=['frequency'],ascending = False)\n",
    "fig = px.bar(df, x = df.term, y = df.frequency)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 16(Take home)\n",
    "mlb = preprocessing.LabelBinarizer()\n",
    "mlb.fit(X.category_name)\n",
    "mlb.classes_\n",
    "X['bin_category_name'] = mlb.transform(X['category_name']).tolist()\n",
    "X[0:9]\n",
    "#Seems work, same result as category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load new datasets\n",
    "import pandas as pd\n",
    "X = pd.read_table('sentiment labelled sentences/amazon_cells_labelled.txt', header=None, names=['sentence','score'])\n",
    "X['source'] = 'amazon'\n",
    "Y = pd.read_table('sentiment labelled sentences/imdb_labelled.txt', header=None, names=['sentence','score'])\n",
    "Y['source'] = 'imdb'\n",
    "Z = pd.read_table('sentiment labelled sentences/yelp_labelled.txt', header=None, names=['sentence','score'])\n",
    "Z['source'] = 'yelp'\n",
    "X = X.append(Y,ignore_index=True).append(Z,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of X\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to print comment of first document\n",
    "print(\"\\n\".join(X.sentence[0].split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in X.score[:10]:\n",
    "    print(X.score[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 1\n",
    "for t in X.sentence[:3]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...skip a bit since I loaded it with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Familiarizing with it\n",
    "X[0:10][[\"sentence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last 10 records\n",
    "X[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iloc\n",
    "X.iloc[::10, 0:2][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loc\n",
    "X.loc[::10, 'sentence'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard query\n",
    "X[::10][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 2\n",
    "#X.iloc[3]   #third line\n",
    "#X.iloc[:3]  #three line\n",
    "#X.iloc[::3] #every three line\n",
    "#X.iloc[3::] #from thrid line\n",
    "X.iloc[3::5]#from third line, every five line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 3\n",
    "X.iloc[::10, : ][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers.data_mining_helpers as dmh\n",
    "X.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 4\n",
    "X.isnull().apply(lambda x: dmh.check_missing_values(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_series = pd.Series([\"dummy_record\"], index=[\"sentence\"])\n",
    "dummy_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_with_series = X.append(dummy_series, ignore_index=True)\n",
    "len(result_with_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_with_series.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_with_series.drop(2748)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy record as dictionary format\n",
    "dummy_dict = [{'score': 0}]\n",
    "X = X.append(dummy_dict, ignore_index=True)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dropna(inplace=True)\n",
    "X.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 5 with new data is exactly same as with old data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(X.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(X.duplicated('sentence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since it has duplicated sentences so I skipped to add dummy record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(X.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Sample = X.sample(n=500)\n",
    "len(X_Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Sample[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.score.value_counts())\n",
    "X.score.value_counts().plot(kind = 'bar',\n",
    "                            title = 'score distribution',\n",
    "                            ylim = [0, 1400],        \n",
    "                            rot = 0, fontsize = 11, figsize = (8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_Sample.score.value_counts())\n",
    "X_Sample.score.value_counts().plot(kind = 'bar',\n",
    "                                   title = 'score distribution',\n",
    "                                   ylim = [0, 300],        \n",
    "                                   rot = 0, fontsize = 11, figsize = (8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7\n",
    "upper_bound = max(X_Sample.score.value_counts())+20\n",
    "X_Sample.score.value_counts().plot(kind = 'bar',\n",
    "                                   title = 'score distribution',\n",
    "                                   ylim = [0, upper_bound],        \n",
    "                                   rot = 0, fontsize = 11, figsize = (8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 8\n",
    "X.score.value_counts().plot(kind = 'bar', position = 1,rot = 0, fontsize = 12, figsize = (8,3))\n",
    "X_Sample.score.value_counts().plot(kind = 'bar', position = 0, color = 'orange' ,rot = 0, fontsize = 12, figsize = (8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "X['unigrams'] = X['sentence'].apply(lambda x: dmh.tokenize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:4][\"unigrams\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(X[0:1]['unigrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(X.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 9 \n",
    "analyze = count_vect.build_analyzer()\n",
    "analyze(\" \".join(list(X[:].sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts[0:5, 0:100].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 10\n",
    "#there is only a 1 in record 4 so I just print it out \n",
    "# Answer here\n",
    "A = X_counts[4].toarray()\n",
    "for i  in range(0,100):\n",
    "    if A[0][i] == 1:\n",
    "        break;\n",
    "count_vect.get_feature_names()[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.transform(['00 Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x = [\"term_\"+str(i) for i in count_vect.get_feature_names()[0:20]]\n",
    "plot_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y = [\"doc_\"+ str(i) for i in list(X.index)[0:20]]\n",
    "plot_z = X_counts[0:20, 0:20].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)\n",
    "plt.subplots(figsize=(9, 7))\n",
    "ax = sns.heatmap(df_todraw,\n",
    "                 cmap=\"PuRd\",\n",
    "                 vmin=0, vmax=1, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 11\n",
    "#Really no idea with it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = PCA(n_components = 2).fit_transform(X_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ['amazon', 'imdb', 'yelp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['coral', 'blue', 'black', 'm']\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize = (25,10))\n",
    "ax = fig.subplots()\n",
    "\n",
    "for c, source in zip(col, source):\n",
    "    xs = X_reduced[X['source'] == source].T[0]\n",
    "    ys = X_reduced[X['source'] == source].T[1]\n",
    "   \n",
    "    ax.scatter(xs, ys, c = c, marker='o')\n",
    "\n",
    "ax.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "ax.set_xlabel('\\nX Label')\n",
    "ax.set_ylabel('\\nY Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 12\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize = (15,10))\n",
    "\n",
    "#ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=60, azim=240)\n",
    "#ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=0, azim=180)\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=0, azim=60)\n",
    "\n",
    "X_reduced = PCA(n_components = 3).fit_transform(X_counts.toarray())\n",
    "source = ['amazon', 'imdb', 'yelp']\n",
    "\n",
    "for c, source in zip(col, source):\n",
    "    xs = X_reduced[X['source'] == source].T[0]\n",
    "    ys = X_reduced[X['source'] == source].T[1]\n",
    "    zs = X_reduced[X['source'] == source].T[2]\n",
    "   \n",
    "    ax.scatter(xs, ys, zs, c = c, marker='o')\n",
    "\n",
    "ax.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "ax.set_xlabel('\\nX Label')\n",
    "ax.set_ylabel('\\nY Label')\n",
    "ax.set_zlabel('\\nZ Label')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#almost every dots are crowded in same space, means that the words left on those website are frequently used, no matter which\n",
    "#site it was left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this takes time to compute. You may want to reduce the amount of terms you want to compute frequencies for\n",
    "term_frequencies = []\n",
    "for j in range(0,X_counts.shape[1]):\n",
    "    term_frequencies.append(sum(X_counts[:,j].toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "term_frequencies = np.asarray(X_counts.sum(axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(frequency=term_frequencies[:], term = count_vect.get_feature_names()[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(100, 10))\n",
    "g = sns.barplot(x=count_vect.get_feature_names()[:300], \n",
    "            y=term_frequencies[:300])\n",
    "g.set_xticklabels(count_vect.get_feature_names()[:300], rotation = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 13\n",
    "\n",
    "import plotly.express as px\n",
    "df = pd.DataFrame(dict(frequency=term_frequencies[:], term = count_vect.get_feature_names()[:]))\n",
    "fig = px.bar(df, x = df.term, y = df.frequency)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 14\n",
    "# dropped some low frequency terms.\n",
    "df = pd.DataFrame(dict(frequency=term_frequencies[:], term = count_vect.get_feature_names()[:]))\n",
    "df = df.drop(df[df.frequency < 10].index)\n",
    "fig = px.bar(df, x = df.term, y = df.frequency)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 15\n",
    "df = pd.DataFrame(dict(frequency=term_frequencies[:], term = count_vect.get_feature_names()[:]))\n",
    "df = df.drop(df[df.frequency < 10].index)\n",
    "df = df.sort_values(by=['frequency'],ascending = False)\n",
    "fig = px.bar(df, x = df.term, y = df.frequency)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "term_frequencies_log = [math.log(i) for i in term_frequencies]\n",
    "plt.subplots(figsize=(100, 10))\n",
    "g = sns.barplot(x=count_vect.get_feature_names()[:300],\n",
    "                y=term_frequencies_log[:300])\n",
    "g.set_xticklabels(count_vect.get_feature_names()[:300], rotation = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, metrics, decomposition, pipeline, dummy\n",
    "mlb = preprocessing.LabelBinarizer()\n",
    "mlb.fit(X.source)\n",
    "X['bin_source'] = mlb.transform(X['source']).tolist()\n",
    "X[2500:2510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I use source to generate binarization and it does work, so I skipped exercise 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrieve 2 sentences for a random record, here, indexed at 1500 and 2500\n",
    "document_to_transform_1 = []\n",
    "random_record_1 = X.iloc[13]\n",
    "random_record_1 = random_record_1['sentence']\n",
    "document_to_transform_1.append(random_record_1)\n",
    "\n",
    "document_to_transform_2 = []\n",
    "random_record_2 = X.iloc[17]\n",
    "random_record_2 = random_record_2['sentence']\n",
    "document_to_transform_2.append(random_record_2)\n",
    "\n",
    "document_to_transform_3 = []\n",
    "random_record_3 = X.iloc[505]\n",
    "random_record_3 = random_record_3['sentence']\n",
    "document_to_transform_3.append(random_record_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document_to_transform_1)\n",
    "print(document_to_transform_2)\n",
    "print(document_to_transform_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "# Transform sentence with Vectorizers\n",
    "document_vector_count_1 = count_vect.transform(document_to_transform_1)\n",
    "document_vector_count_2 = count_vect.transform(document_to_transform_2)\n",
    "document_vector_count_3 = count_vect.transform(document_to_transform_3)\n",
    "\n",
    "# Binarize vecors to simplify: 0 for abscence, 1 for prescence\n",
    "document_vector_count_1_bin = binarize(document_vector_count_1)\n",
    "document_vector_count_2_bin = binarize(document_vector_count_2)\n",
    "document_vector_count_3_bin = binarize(document_vector_count_3)\n",
    "\n",
    "# print\n",
    "print(\"Let's take a look at the count vectors:\")\n",
    "print(document_vector_count_1.todense())\n",
    "print(document_vector_count_2.todense())\n",
    "print(document_vector_count_3.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate Cosine Similarity\n",
    "cos_sim_count_1_2 = cosine_similarity(document_vector_count_1, document_vector_count_2, dense_output=True)\n",
    "cos_sim_count_1_3 = cosine_similarity(document_vector_count_1, document_vector_count_3, dense_output=True)\n",
    "cos_sim_count_1_1 = cosine_similarity(document_vector_count_1, document_vector_count_1, dense_output=True)\n",
    "cos_sim_count_2_2 = cosine_similarity(document_vector_count_2, document_vector_count_2, dense_output=True)\n",
    "\n",
    "# Print \n",
    "print(\"Cosine Similarity using count bw 1 and 2: %(x)f\" %{\"x\":cos_sim_count_1_2})\n",
    "print(\"Cosine Similarity using count bw 1 and 3: %(x)f\" %{\"x\":cos_sim_count_1_3})\n",
    "print(\"Cosine Similarity using count bw 1 and 1: %(x)f\" %{\"x\":cos_sim_count_1_1})\n",
    "print(\"Cosine Similarity using count bw 2 and 2: %(x)f\" %{\"x\":cos_sim_count_2_2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate dataframe by score\n",
    "Pos = X[X['score'] == 1]\n",
    "Neg = X[X['score'] == 0]\n",
    "\n",
    "Pos_term_frequencies = []\n",
    "Neg_term_frequencies = []\n",
    "\n",
    "Pos_counts = count_vect.fit_transform(Pos.sentence)\n",
    "for j in range(0,Pos_counts.shape[1]):\n",
    "    Pos_term_frequencies.append(sum(Pos_counts[:,j].toarray()))\n",
    "Pos_term_frequencies = np.asarray(Pos_counts.sum(axis=0))[0]\n",
    "Posdf = pd.DataFrame(dict(frequency=Pos_term_frequencies[:], term = count_vect.get_feature_names()[:]))\n",
    "\n",
    "Neg_counts = count_vect.fit_transform(Neg.sentence)\n",
    "for j in range(0,Neg_counts.shape[1]):\n",
    "    Neg_term_frequencies.append(sum(Neg_counts[:,j].toarray()))\n",
    "Neg_term_frequencies = np.asarray(Neg_counts.sum(axis=0))[0]\n",
    "Negdf = pd.DataFrame(dict(frequency=Neg_term_frequencies[:], term = count_vect.get_feature_names()[:]))\n",
    "\n",
    "Posdf['color'] = 'coral'\n",
    "Negdf['color'] = 'blue'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the term appears in positive sentiment more or in negative more \n",
    "DF = pd.concat((Posdf,Negdf),ignore_index=True)\n",
    "px.bar(DF, x='term', y='frequency', color='color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf part\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "vectorizer = CountVectorizer()  \n",
    "f = vectorizer.fit_transform(X.sentence)\n",
    "f\n",
    "r = pd.DataFrame(f.toarray(),columns=vectorizer.get_feature_names())\n",
    "print(\"CountVector\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=True)\n",
    "Z = transformer.fit_transform(f)\n",
    "r = pd.DataFrame(Z.toarray(),columns=vectorizer.get_feature_names())\n",
    "print(\"TFIDF\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayes classifier - word frequency\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(X['sentence'])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(X['source'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3)\n",
    "\n",
    "mlt = MultinomialNB()\n",
    "mlt.fit(x_train, y_train)\n",
    "y_predict = mlt.predict(x_test)\n",
    "\n",
    "print(\"acc：\",mlt.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bayes classifier - tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X.sentence,X.source,test_size=0.3)\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "x_train = tf.fit_transform(x_train)\n",
    "x_test = tf.transform(x_test)\n",
    "\n",
    "mlt = MultinomialNB(alpha=1.0)\n",
    "mlt.fit(x_train,y_train)\n",
    "y_predict = mlt.predict(x_test)\n",
    "\n",
    "print(\"acc：\",mlt.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Tried several times and found that word frequency based classifier acheives slightly higher accuracy as 84 ~ 89% rather than\n",
    "tf-idf based one, which has about 82 ~ 87%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dont really understand why do we need category while we have category_name, they basically serve for samething.\n",
    "Another point is some of the columns were'nt used again after the section it was added, maybe we can remove it after the section, otherwise the table gonna become messy over several sections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
